{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypR4NNY7oyEV"
   },
   "source": [
    "#### We need to install the ktrain library. Its a light weight wrapper for keras to help train neural networks. With only a few lines of code it allows you to build models, estimate optimal learning rate, loading and preprocessing text and image data from various sources and much more. More about our approach can be found at [this](https://towardsdatascience.com/bert-text-classification-in-3-lines-of-code-using-keras-264db7e7a358) article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TF5qfV_flTbr",
    "outputId": "b536d10d-767d-4a8d-9cd6-2ea607550b1b"
   },
   "outputs": [],
   "source": [
    "# To install only the requirements of this notebook, uncomment the lines below and run this cell\n",
    "\n",
    "# ===========================\n",
    "\n",
    "# !pip install numpy==1.19.5\n",
    "# !pip install pandas==1.1.5\n",
    "# !pip install ktrain==0.26.3\n",
    "\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UN7tuqnlTbs"
   },
   "outputs": [],
   "source": [
    "# To install the requirements for the entire chapter, uncomment the lines below and run this cell\n",
    "\n",
    "# ===========================\n",
    "\n",
    "# try:\n",
    "#     import google.colab\n",
    "#     !curl  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/ch4-requirements.txt | xargs -n 1 -L 1 pip install\n",
    "# except ModuleNotFoundError:\n",
    "#     !pip install -r \"ch4-requirements.txt\"\n",
    "\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58WB13Jx3rQm",
    "outputId": "9af6cd3f-771e-4807-d041-bb8a3290bea1"
   },
   "outputs": [],
   "source": [
    "# use tensorflow 2.4.0 for this notebook\n",
    "# !pip install tensorflow==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KN6N85ah8VXf"
   },
   "outputs": [],
   "source": [
    "#Importing\n",
    "import ktrain\n",
    "from ktrain import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mr1YXudk8Vti",
    "outputId": "4634f5ee-9c9d-4a32-9118-1845b6c43b7f"
   },
   "outputs": [],
   "source": [
    "##obtain the dataset\n",
    "import os\n",
    "try :\n",
    "    from google.colab import files\n",
    "    import tensorflow as tf\n",
    "    dataset = tf.keras.utils.get_file(\n",
    "        fname=\"aclImdb.tar.gz\", \n",
    "        origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "        extract=True,\n",
    "    )\n",
    "    IMDB_DATADIR = os.path.join(os.path.dirname(dataset), \"aclImdb\")\n",
    "except ModuleNotFoundError :\n",
    "    pwd = os.getcwd()\n",
    "    file_path = os.path.join(pwd, 'Data', 'aclImdb')\n",
    "    if not os.path.exists(file_path) :\n",
    "        import tensorflow as tf\n",
    "        dataset = tf.keras.utils.get_file(\n",
    "            fname=\"aclImdb.tar.gz\", \n",
    "            origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "            extract=True,\n",
    "        )\n",
    "\n",
    "        # set path to dataset\n",
    "        IMDB_DATADIR=pwd\n",
    "    else :\n",
    "\n",
    "        # set path to dataset\n",
    "        IMDB_DATADIR=file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugopbOABrmne"
   },
   "source": [
    "## STEP 1: Preprocessing\n",
    "The texts_from_folder function will load the training and validation data from the specified folder and automatically preprocess it according to BERT's requirements. In doing so, the BERT model and vocabulary will be automatically downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "jELdxonN9J8v",
    "outputId": "89eaacb7-9b62-4cca-e7df-0992cea04e7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: utf-8\n",
      "downloading pretrained BERT model (uncased_L-12_H-768_A-12.zip)...\n",
      "[██████████████████████████████████████████████████]\n",
      "extracting pretrained BERT model...\n",
      "done.\n",
      "\n",
      "cleanup downloaded zip...\n",
      "done.\n",
      "\n",
      "preprocessing train...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "preprocessing test...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test), preproc = text.texts_from_folder (IMDB_DATADIR,                                                              maxlen=500,                                                                    preprocess_mode='bert',                                                         train_test_names=['train', 'test'],\n",
    "    classes=['pos', 'neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0SIaqHcslLZ"
   },
   "source": [
    "### STEP 2: Loading a pre trained BERT and wrapping it in a ktrain.learner object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90ftQ6MgAJy4",
    "outputId": "907724f8-61d3-463c-aef5-114721f15cb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "maxlen is 500\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "model = text.text_classifier('bert', (x_train, y_train), preproc=preproc)\n",
    "learner = ktrain.get_learner(model,train_data=(x_train, y_train), val_data=(x_test, y_test), batch_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nN6zWQgys0c_"
   },
   "source": [
    "### STEP 3: Training and Tuning the model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fxdw88YjAfvF",
    "outputId": "8296ec00-3e48-4759-fc76-e118f2bd48da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 2e-05...\n",
      "Epoch 1/4\n",
      "  52/4167 [..............................] - ETA: 35:17:25 - loss: 0.6890 - accuracy: 0.5385"
     ]
    }
   ],
   "source": [
    "learner.fit_onecycle(2e-5, 4)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "07_BERT_Sentiment_Classification_IMDB_ktrain.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
